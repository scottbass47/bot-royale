\hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep}{}\doxysection{Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep Class Reference}
\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep}\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}


A very close approximation of the logistic function that avoids use of exp() and is therefore typically much faster to compute, while giving an almost identical sigmoid curve.  


Inheritance diagram for Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep}
\end{center}
\end{figure}
\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
double \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a71248e2a0310b18115a56060802b8a0d}{Calculate}} (double x, double\mbox{[}$\,$\mbox{]} aux\+Args)
\begin{DoxyCompactList}\small\item\em Calculates the output value for the specified input value and optional activation function auxiliary arguments. \end{DoxyCompactList}\item 
float \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_ae4cc5b89b8c49359e2de007d51f543aa}{Calculate}} (float x, float\mbox{[}$\,$\mbox{]} aux\+Args)
\begin{DoxyCompactList}\small\item\em Calculates the output value for the specified input value and optional activation function auxiliary arguments. This single precision overload of \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a71248e2a0310b18115a56060802b8a0d}{Calculate()}} will be used in neural network code that has been specifically written to use floats instead of doubles. \end{DoxyCompactList}\item 
double\mbox{[}$\,$\mbox{]} \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a26e2bce98543337bda5679d1597b9cff}{Get\+Random\+Aux\+Args}} (I\+Random\+Source rng, double connection\+Weight\+Range)
\begin{DoxyCompactList}\small\item\em For activation functions that accept auxiliary arguments; generates random initial values for aux arguments for newly added nodes (from an \textquotesingle{}add neuron\textquotesingle{} mutation). \end{DoxyCompactList}\item 
void \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a6baa1a15140c0d49f48ab2822bca4466}{Mutate\+Aux\+Args}} (double\mbox{[}$\,$\mbox{]} aux\+Args, I\+Random\+Source rng, double connection\+Weight\+Range)
\begin{DoxyCompactList}\small\item\em Genetic mutation for auxiliary argument data. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsection*{Static Public Attributes}
\begin{DoxyCompactItemize}
\item 
static readonly \mbox{\hyperlink{interface_sharp_neat_1_1_network_1_1_i_activation_function}{I\+Activation\+Function}} \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a889dc997045d724d7a26555c8ddf0b5a}{\+\_\+\+\_\+\+Default\+Instance}} = new \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep}{Polynomial\+Approximant\+Steep}}()
\begin{DoxyCompactList}\small\item\em Default instance provided as a public static field. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsection*{Properties}
\begin{DoxyCompactItemize}
\item 
string \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_acdeb360732c39e11ac058164a7ef4383}{Function\+Id}}\hspace{0.3cm}{\ttfamily  \mbox{[}get\mbox{]}}
\begin{DoxyCompactList}\small\item\em Gets the unique ID of the function. Stored in network X\+ML to identify which function a network or neuron is using. \end{DoxyCompactList}\item 
string \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a997f9b3411196d006ee04be3c44aa1b7}{Function\+String}}\hspace{0.3cm}{\ttfamily  \mbox{[}get\mbox{]}}
\begin{DoxyCompactList}\small\item\em Gets a human readable string representation of the function. E.\+g \textquotesingle{}y=1/x\textquotesingle{}. \end{DoxyCompactList}\item 
string \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a4323814f871dbb14faab00e00f11cd95}{Function\+Description}}\hspace{0.3cm}{\ttfamily  \mbox{[}get\mbox{]}}
\begin{DoxyCompactList}\small\item\em Gets a human readable verbose description of the activation function. \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a784b4d3fe1a28a84c358cd309ee496e5}{Accepts\+Aux\+Args}}\hspace{0.3cm}{\ttfamily  \mbox{[}get\mbox{]}}
\begin{DoxyCompactList}\small\item\em Gets a flag that indicates if the activation function accepts auxiliary arguments. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
A very close approximation of the logistic function that avoids use of exp() and is therefore typically much faster to compute, while giving an almost identical sigmoid curve. 

This function was obtained from\+: \href{http://stackoverflow.com/a/34448562/15703}{\texttt{ http\+://stackoverflow.\+com/a/34448562/15703}}

This might be based on the Pade approximant\+: \href{https://en.wikipedia.org/wiki/Pad\%C3\%A9_approximant}{\texttt{ https\+://en.\+wikipedia.\+org/wiki/\+Pad\%\+C3\%\+A9\+\_\+approximant}} \href{https://math.stackexchange.com/a/107666}{\texttt{ https\+://math.\+stackexchange.\+com/a/107666}}

Or perhaps the maple minimax approximation\+: \href{http://www.maplesoft.com/support/helpJP/Maple/view.aspx?path=numapprox/minimax}{\texttt{ http\+://www.\+maplesoft.\+com/support/help\+J\+P/\+Maple/view.\+aspx?path=numapprox/minimax}}

This is a variant that has a steeper slope at and around the origin that is intended to be a similar slope to that of \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_logistic_function_steep}{Logistic\+Function\+Steep}}.

\doxysubsection{Member Function Documentation}
\mbox{\Hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a71248e2a0310b18115a56060802b8a0d}\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a71248e2a0310b18115a56060802b8a0d}} 
\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}!Calculate@{Calculate}}
\index{Calculate@{Calculate}!SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}
\doxysubsubsection{\texorpdfstring{Calculate()}{Calculate()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily double Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep.\+Calculate (\begin{DoxyParamCaption}\item[{double}]{x,  }\item[{double\mbox{[}$\,$\mbox{]}}]{aux\+Args }\end{DoxyParamCaption})}



Calculates the output value for the specified input value and optional activation function auxiliary arguments. 



Implements \mbox{\hyperlink{interface_sharp_neat_1_1_network_1_1_i_activation_function_a6230550f91b00cf7ac384e8a885dd11a}{Sharp\+Neat.\+Network.\+I\+Activation\+Function}}.

\mbox{\Hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_ae4cc5b89b8c49359e2de007d51f543aa}\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_ae4cc5b89b8c49359e2de007d51f543aa}} 
\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}!Calculate@{Calculate}}
\index{Calculate@{Calculate}!SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}
\doxysubsubsection{\texorpdfstring{Calculate()}{Calculate()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily float Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep.\+Calculate (\begin{DoxyParamCaption}\item[{float}]{x,  }\item[{float\mbox{[}$\,$\mbox{]}}]{aux\+Args }\end{DoxyParamCaption})}



Calculates the output value for the specified input value and optional activation function auxiliary arguments. This single precision overload of \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a71248e2a0310b18115a56060802b8a0d}{Calculate()}} will be used in neural network code that has been specifically written to use floats instead of doubles. 



Implements \mbox{\hyperlink{interface_sharp_neat_1_1_network_1_1_i_activation_function_a17e6d971ff650f3f9ab7863ce0e555c3}{Sharp\+Neat.\+Network.\+I\+Activation\+Function}}.

\mbox{\Hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a26e2bce98543337bda5679d1597b9cff}\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a26e2bce98543337bda5679d1597b9cff}} 
\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}!GetRandomAuxArgs@{GetRandomAuxArgs}}
\index{GetRandomAuxArgs@{GetRandomAuxArgs}!SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}
\doxysubsubsection{\texorpdfstring{GetRandomAuxArgs()}{GetRandomAuxArgs()}}
{\footnotesize\ttfamily double \mbox{[}$\,$\mbox{]} Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep.\+Get\+Random\+Aux\+Args (\begin{DoxyParamCaption}\item[{I\+Random\+Source}]{rng,  }\item[{double}]{connection\+Weight\+Range }\end{DoxyParamCaption})}



For activation functions that accept auxiliary arguments; generates random initial values for aux arguments for newly added nodes (from an \textquotesingle{}add neuron\textquotesingle{} mutation). 



Implements \mbox{\hyperlink{interface_sharp_neat_1_1_network_1_1_i_activation_function_a6bf5c2eb7a6b09deb280a6e3653700db}{Sharp\+Neat.\+Network.\+I\+Activation\+Function}}.

\mbox{\Hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a6baa1a15140c0d49f48ab2822bca4466}\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a6baa1a15140c0d49f48ab2822bca4466}} 
\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}!MutateAuxArgs@{MutateAuxArgs}}
\index{MutateAuxArgs@{MutateAuxArgs}!SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}
\doxysubsubsection{\texorpdfstring{MutateAuxArgs()}{MutateAuxArgs()}}
{\footnotesize\ttfamily void Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep.\+Mutate\+Aux\+Args (\begin{DoxyParamCaption}\item[{double\mbox{[}$\,$\mbox{]}}]{aux\+Args,  }\item[{I\+Random\+Source}]{rng,  }\item[{double}]{connection\+Weight\+Range }\end{DoxyParamCaption})}



Genetic mutation for auxiliary argument data. 



Implements \mbox{\hyperlink{interface_sharp_neat_1_1_network_1_1_i_activation_function_a2943116b401f4dff51448228ec9a6b71}{Sharp\+Neat.\+Network.\+I\+Activation\+Function}}.



\doxysubsection{Member Data Documentation}
\mbox{\Hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a889dc997045d724d7a26555c8ddf0b5a}\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a889dc997045d724d7a26555c8ddf0b5a}} 
\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}!\_\_DefaultInstance@{\_\_DefaultInstance}}
\index{\_\_DefaultInstance@{\_\_DefaultInstance}!SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}
\doxysubsubsection{\texorpdfstring{\_\_DefaultInstance}{\_\_DefaultInstance}}
{\footnotesize\ttfamily readonly \mbox{\hyperlink{interface_sharp_neat_1_1_network_1_1_i_activation_function}{I\+Activation\+Function}} Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep.\+\_\+\+\_\+\+Default\+Instance = new \mbox{\hyperlink{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep}{Polynomial\+Approximant\+Steep}}()\hspace{0.3cm}{\ttfamily [static]}}



Default instance provided as a public static field. 



\doxysubsection{Property Documentation}
\mbox{\Hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a784b4d3fe1a28a84c358cd309ee496e5}\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a784b4d3fe1a28a84c358cd309ee496e5}} 
\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}!AcceptsAuxArgs@{AcceptsAuxArgs}}
\index{AcceptsAuxArgs@{AcceptsAuxArgs}!SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}
\doxysubsubsection{\texorpdfstring{AcceptsAuxArgs}{AcceptsAuxArgs}}
{\footnotesize\ttfamily bool Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep.\+Accepts\+Aux\+Args\hspace{0.3cm}{\ttfamily [get]}}



Gets a flag that indicates if the activation function accepts auxiliary arguments. 

\mbox{\Hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a4323814f871dbb14faab00e00f11cd95}\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a4323814f871dbb14faab00e00f11cd95}} 
\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}!FunctionDescription@{FunctionDescription}}
\index{FunctionDescription@{FunctionDescription}!SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}
\doxysubsubsection{\texorpdfstring{FunctionDescription}{FunctionDescription}}
{\footnotesize\ttfamily string Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep.\+Function\+Description\hspace{0.3cm}{\ttfamily [get]}}



Gets a human readable verbose description of the activation function. 

\mbox{\Hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_acdeb360732c39e11ac058164a7ef4383}\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_acdeb360732c39e11ac058164a7ef4383}} 
\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}!FunctionId@{FunctionId}}
\index{FunctionId@{FunctionId}!SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}
\doxysubsubsection{\texorpdfstring{FunctionId}{FunctionId}}
{\footnotesize\ttfamily string Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep.\+Function\+Id\hspace{0.3cm}{\ttfamily [get]}}



Gets the unique ID of the function. Stored in network X\+ML to identify which function a network or neuron is using. 

\mbox{\Hypertarget{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a997f9b3411196d006ee04be3c44aa1b7}\label{class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep_a997f9b3411196d006ee04be3c44aa1b7}} 
\index{SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}!FunctionString@{FunctionString}}
\index{FunctionString@{FunctionString}!SharpNeat.Network.PolynomialApproximantSteep@{SharpNeat.Network.PolynomialApproximantSteep}}
\doxysubsubsection{\texorpdfstring{FunctionString}{FunctionString}}
{\footnotesize\ttfamily string Sharp\+Neat.\+Network.\+Polynomial\+Approximant\+Steep.\+Function\+String\hspace{0.3cm}{\ttfamily [get]}}



Gets a human readable string representation of the function. E.\+g \textquotesingle{}y=1/x\textquotesingle{}. 



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/\+Sharp\+Neat\+Lib/\+Network/\+Activation\+Functions/\+Unipolar/Polynomial\+Approximant\+Steep.\+cs\end{DoxyCompactItemize}
