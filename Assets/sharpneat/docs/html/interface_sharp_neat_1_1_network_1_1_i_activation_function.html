<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>SharpNEAT: SharpNeat.Network.IActivationFunction Interface Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">SharpNEAT
   </div>
   <div id="projectbrief">C# implementation of NEAT</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('interface_sharp_neat_1_1_network_1_1_i_activation_function.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#properties">Properties</a> &#124;
<a href="interface_sharp_neat_1_1_network_1_1_i_activation_function-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">SharpNeat.Network.IActivationFunction Interface Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Interface for neural network activation functions. An activation function simply takes a single input value and produces a single output value. <a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html" title="Interface for neural network activation functions. An activation function simply takes a single input...">IActivationFunction</a> allows for activation functions to be plugged in to neural network implementations. Typical activation functions would be a sigmoid or step function.  
 <a href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for SharpNeat.Network.IActivationFunction:</div>
<div class="dyncontent">
 <div class="center">
  <img src="interface_sharp_neat_1_1_network_1_1_i_activation_function.png" usemap="#SharpNeat.Network.IActivationFunction_map" alt=""/>
  <map id="SharpNeat.Network.IActivationFunction_map" name="SharpNeat.Network.IActivationFunction_map">
<area href="class_sharp_neat_1_1_network_1_1_arc_sin_h.html" title="Rectified linear activation unit (ReLU)." alt="SharpNeat.Network.ArcSinH" shape="rect" coords="299,56,588,80"/>
<area href="class_sharp_neat_1_1_network_1_1_arc_tan.html" alt="SharpNeat.Network.ArcTan" shape="rect" coords="299,112,588,136"/>
<area href="class_sharp_neat_1_1_network_1_1_bipolar_gaussian.html" title="Bipolar Gaussian activation function. Output range is -1 to 1, that is, the tails of the Gaussian dis..." alt="SharpNeat.Network.BipolarGaussian" shape="rect" coords="299,168,588,192"/>
<area href="class_sharp_neat_1_1_network_1_1_bipolar_sigmoid.html" title="Bipolar sigmoid activation function. Output range is -1 to 1 instead of the more normal 0 to 1." alt="SharpNeat.Network.BipolarSigmoid" shape="rect" coords="299,224,588,248"/>
<area href="class_sharp_neat_1_1_network_1_1_gaussian.html" title="Gaussian activation function. Output range is 0 to 1, that is, the tails of the Gaussian distribution..." alt="SharpNeat.Network.Gaussian" shape="rect" coords="299,280,588,304"/>
<area href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u.html" title="Leaky rectified linear activation unit (ReLU)." alt="SharpNeat.Network.LeakyReLU" shape="rect" coords="299,336,588,360"/>
<area href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u_shifted.html" title="Leaky rectified linear activation unit (ReLU). Shifted on the x-axis so that x=0 gives y=0...." alt="SharpNeat.Network.LeakyReLUShifted" shape="rect" coords="299,392,588,416"/>
<area href="class_sharp_neat_1_1_network_1_1_linear.html" title="Linear activation function with clipping. By &#39;clipping&#39; we mean the output value is linear between x ..." alt="SharpNeat.Network.Linear" shape="rect" coords="299,448,588,472"/>
<area href="class_sharp_neat_1_1_network_1_1_logistic_function.html" title="The logistic function." alt="SharpNeat.Network.LogisticFunction" shape="rect" coords="299,504,588,528"/>
<area href="class_sharp_neat_1_1_network_1_1_logistic_function_steep.html" title="The logistic function with a steepened slope." alt="SharpNeat.Network.LogisticFunctionSteep" shape="rect" coords="299,560,588,584"/>
<area href="class_sharp_neat_1_1_network_1_1_max_minus_one.html" title="y = max(-1, x). i.e. a rectified linear activation unit (ReLU) variant." alt="SharpNeat.Network.MaxMinusOne" shape="rect" coords="299,616,588,640"/>
<area href="class_sharp_neat_1_1_network_1_1_null_fn.html" title="Null activation function. Returns zero regardless of input." alt="SharpNeat.Network.NullFn" shape="rect" coords="299,672,588,696"/>
<area href="class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep.html" title="A very close approximation of the logistic function that avoids use of exp() and is therefore typical..." alt="SharpNeat.Network.PolynomialApproximantSteep" shape="rect" coords="299,728,588,752"/>
<area href="class_sharp_neat_1_1_network_1_1_quadratic_sigmoid.html" title="A sigmoid formed by two sub-sections of the y=x^2 curve." alt="SharpNeat.Network.QuadraticSigmoid" shape="rect" coords="299,784,588,808"/>
<area href="class_sharp_neat_1_1_network_1_1_rbf_gaussian.html" title="Gaussian activation function. Output range is 0 to 1, that is, the tails of the Gaussian distribution..." alt="SharpNeat.Network.RbfGaussian" shape="rect" coords="299,840,588,864"/>
<area href="class_sharp_neat_1_1_network_1_1_re_l_u.html" title="Rectified linear activation unit (ReLU)." alt="SharpNeat.Network.ReLU" shape="rect" coords="299,896,588,920"/>
<area href="class_sharp_neat_1_1_network_1_1_scaled_e_l_u.html" title="Scaled Exponential Linear Unit (SELU)." alt="SharpNeat.Network.ScaledELU" shape="rect" coords="299,952,588,976"/>
<area href="class_sharp_neat_1_1_network_1_1_sine.html" title="Sine activation function with doubled period." alt="SharpNeat.Network.Sine" shape="rect" coords="299,1008,588,1032"/>
<area href="class_sharp_neat_1_1_network_1_1_soft_sign_steep.html" title="The softsign sigmoid. This is a variant of softsign that has a steeper slope at and around the origin..." alt="SharpNeat.Network.SoftSignSteep" shape="rect" coords="299,1064,588,1088"/>
<area href="class_sharp_neat_1_1_network_1_1_s_re_l_u.html" title="S-shaped rectified linear activation unit (SReLU). From: https://en.wikipedia.org/wiki/Activation_fun..." alt="SharpNeat.Network.SReLU" shape="rect" coords="299,1120,588,1144"/>
<area href="class_sharp_neat_1_1_network_1_1_s_re_l_u_shifted.html" title="S-shaped rectified linear activation unit (SReLU). Shifted on the x-axis so that x=0 gives y=0...." alt="SharpNeat.Network.SReLUShifted" shape="rect" coords="299,1176,588,1200"/>
<area href="class_sharp_neat_1_1_network_1_1_tan_h.html" alt="SharpNeat.Network.TanH" shape="rect" coords="299,1232,588,1256"/>
  </map>
</div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a6230550f91b00cf7ac384e8a885dd11a"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a6230550f91b00cf7ac384e8a885dd11a">Calculate</a> (double x, double[] auxArgs)</td></tr>
<tr class="memdesc:a6230550f91b00cf7ac384e8a885dd11a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Calculates the output value for the specified input value and optional activation function auxiliary arguments.  <a href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a6230550f91b00cf7ac384e8a885dd11a">More...</a><br /></td></tr>
<tr class="separator:a6230550f91b00cf7ac384e8a885dd11a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a17e6d971ff650f3f9ab7863ce0e555c3"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a17e6d971ff650f3f9ab7863ce0e555c3">Calculate</a> (float x, float[] auxArgs)</td></tr>
<tr class="memdesc:a17e6d971ff650f3f9ab7863ce0e555c3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Calculates the output value for the specified input value and optional activation function auxiliary arguments. This single precision overload of <a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a6230550f91b00cf7ac384e8a885dd11a" title="Calculates the output value for the specified input value and optional activation function auxiliary ...">Calculate()</a> will be used in neural network code that has been specifically written to use floats instead of doubles.  <a href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a17e6d971ff650f3f9ab7863ce0e555c3">More...</a><br /></td></tr>
<tr class="separator:a17e6d971ff650f3f9ab7863ce0e555c3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6bf5c2eb7a6b09deb280a6e3653700db"><td class="memItemLeft" align="right" valign="top">double[]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a6bf5c2eb7a6b09deb280a6e3653700db">GetRandomAuxArgs</a> (IRandomSource rng, double connectionWeightRange)</td></tr>
<tr class="memdesc:a6bf5c2eb7a6b09deb280a6e3653700db"><td class="mdescLeft">&#160;</td><td class="mdescRight">For activation functions that accept auxiliary arguments; generates random initial values for aux arguments for newly added nodes (from an 'add neuron' mutation).  <a href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a6bf5c2eb7a6b09deb280a6e3653700db">More...</a><br /></td></tr>
<tr class="separator:a6bf5c2eb7a6b09deb280a6e3653700db"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2943116b401f4dff51448228ec9a6b71"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a2943116b401f4dff51448228ec9a6b71">MutateAuxArgs</a> (double[] auxArgs, IRandomSource rng, double connectionWeightRange)</td></tr>
<tr class="memdesc:a2943116b401f4dff51448228ec9a6b71"><td class="mdescLeft">&#160;</td><td class="mdescRight">Genetic mutation for auxiliary argument data.  <a href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a2943116b401f4dff51448228ec9a6b71">More...</a><br /></td></tr>
<tr class="separator:a2943116b401f4dff51448228ec9a6b71"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="properties"></a>
Properties</h2></td></tr>
<tr class="memitem:a63c74ef293a625aa7e35ce9d6dea7e2d"><td class="memItemLeft" align="right" valign="top">string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a63c74ef293a625aa7e35ce9d6dea7e2d">FunctionId</a><code> [get]</code></td></tr>
<tr class="memdesc:a63c74ef293a625aa7e35ce9d6dea7e2d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the unique ID of the function. Stored in network XML to identify which function a network or neuron is using.  <a href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a63c74ef293a625aa7e35ce9d6dea7e2d">More...</a><br /></td></tr>
<tr class="separator:a63c74ef293a625aa7e35ce9d6dea7e2d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad9f4086a305d232a1f099eba8794293a"><td class="memItemLeft" align="right" valign="top">string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#ad9f4086a305d232a1f099eba8794293a">FunctionString</a><code> [get]</code></td></tr>
<tr class="memdesc:ad9f4086a305d232a1f099eba8794293a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets a human readable string representation of the function. E.g 'y=1/x'.  <a href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#ad9f4086a305d232a1f099eba8794293a">More...</a><br /></td></tr>
<tr class="separator:ad9f4086a305d232a1f099eba8794293a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5581e8b834a77292473b0ea23c551302"><td class="memItemLeft" align="right" valign="top">string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a5581e8b834a77292473b0ea23c551302">FunctionDescription</a><code> [get]</code></td></tr>
<tr class="memdesc:a5581e8b834a77292473b0ea23c551302"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets a human readable verbose description of the activation function.  <a href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a5581e8b834a77292473b0ea23c551302">More...</a><br /></td></tr>
<tr class="separator:a5581e8b834a77292473b0ea23c551302"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad5ff022024f524777ce2c6df1ff04672"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#ad5ff022024f524777ce2c6df1ff04672">AcceptsAuxArgs</a><code> [get]</code></td></tr>
<tr class="memdesc:ad5ff022024f524777ce2c6df1ff04672"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets a flag that indicates if the activation function accepts auxiliary arguments.  <a href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#ad5ff022024f524777ce2c6df1ff04672">More...</a><br /></td></tr>
<tr class="separator:ad5ff022024f524777ce2c6df1ff04672"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Interface for neural network activation functions. An activation function simply takes a single input value and produces a single output value. <a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html" title="Interface for neural network activation functions. An activation function simply takes a single input...">IActivationFunction</a> allows for activation functions to be plugged in to neural network implementations. Typical activation functions would be a sigmoid or step function. </p>
<p>The Calculate method has two overloads, one for each of the data types double and float, this allows an <a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html" title="Interface for neural network activation functions. An activation function simply takes a single input...">IActivationFunction</a> object to be plugged in to neural network classes that are written to operate with either of those two data types. Typically the choice of which neural network implementation and floating point precision to use is part of the setting up and design of a problem domain and experiment. For some problem domains the extra precision of a double may be unnecessary.</p>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a id="a6230550f91b00cf7ac384e8a885dd11a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6230550f91b00cf7ac384e8a885dd11a">&#9670;&nbsp;</a></span>Calculate() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double SharpNeat.Network.IActivationFunction.Calculate </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double[]&#160;</td>
          <td class="paramname"><em>auxArgs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Calculates the output value for the specified input value and optional activation function auxiliary arguments. </p>

<p>Implemented in <a class="el" href="class_sharp_neat_1_1_network_1_1_rbf_gaussian.html#a13ba8bac5bb060ee436ae4c4a8117a8a">SharpNeat.Network.RbfGaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep.html#a71248e2a0310b18115a56060802b8a0d">SharpNeat.Network.PolynomialApproximantSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_quadratic_sigmoid.html#a3282e971aba728f564f9f15182f314de">SharpNeat.Network.QuadraticSigmoid</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_soft_sign_steep.html#a3cb1bcb3f35732874a42b507f3203fd7">SharpNeat.Network.SoftSignSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_bipolar_gaussian.html#aa24fa03c73015b7e937a4341d7d77736">SharpNeat.Network.BipolarGaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_gaussian.html#a329fb4012f4c63592e7d3eef3aceba55">SharpNeat.Network.Gaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_bipolar_sigmoid.html#a837c4269f092b93ddf7433a6dc3bf622">SharpNeat.Network.BipolarSigmoid</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_linear.html#a724d8bb23211b76bc4f44792b586d99a">SharpNeat.Network.Linear</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_sine.html#a3894f8ea9942f3cfeea0863670b301e3">SharpNeat.Network.Sine</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_logistic_function.html#a3e30ce9a829d6d5f172cebc95bfb9241">SharpNeat.Network.LogisticFunction</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_logistic_function_steep.html#a38f7d9e627117879c18587969325d9bd">SharpNeat.Network.LogisticFunctionSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_null_fn.html#a672a5eea83267fbe1274a1539c314cb1">SharpNeat.Network.NullFn</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_scaled_e_l_u.html#ae5104615f87eadcbe27704e47b982547">SharpNeat.Network.ScaledELU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_s_re_l_u.html#a4612230cdbd52da8c8cf08bc8b560015">SharpNeat.Network.SReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_s_re_l_u_shifted.html#aab5b1d6f6a56e23686168dc4d9b60ce0">SharpNeat.Network.SReLUShifted</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_arc_sin_h.html#a30c70469017ca89851142a6d8831ecb5">SharpNeat.Network.ArcSinH</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u_shifted.html#a569ae6c68f04405310952bb0418eb23a">SharpNeat.Network.LeakyReLUShifted</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_max_minus_one.html#af30adce5e873b64c3b307991decb497a">SharpNeat.Network.MaxMinusOne</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u.html#a2419d9f8f8b3f2435c8cf10636ce5e39">SharpNeat.Network.LeakyReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_re_l_u.html#a2035a931b6b4c74d463a6a73b5f5df4c">SharpNeat.Network.ReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_arc_tan.html#a538565fcde220fe51da4f3ec90a90b1b">SharpNeat.Network.ArcTan</a>, and <a class="el" href="class_sharp_neat_1_1_network_1_1_tan_h.html#aa1d79f456b07afaedef7f2c90c40dbb1">SharpNeat.Network.TanH</a>.</p>

</div>
</div>
<a id="a17e6d971ff650f3f9ab7863ce0e555c3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a17e6d971ff650f3f9ab7863ce0e555c3">&#9670;&nbsp;</a></span>Calculate() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">float SharpNeat.Network.IActivationFunction.Calculate </td>
          <td>(</td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float[]&#160;</td>
          <td class="paramname"><em>auxArgs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Calculates the output value for the specified input value and optional activation function auxiliary arguments. This single precision overload of <a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html#a6230550f91b00cf7ac384e8a885dd11a" title="Calculates the output value for the specified input value and optional activation function auxiliary ...">Calculate()</a> will be used in neural network code that has been specifically written to use floats instead of doubles. </p>

<p>Implemented in <a class="el" href="class_sharp_neat_1_1_network_1_1_rbf_gaussian.html#a8df1e079d9e7240b568b9302b9869cff">SharpNeat.Network.RbfGaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep.html#ae4cc5b89b8c49359e2de007d51f543aa">SharpNeat.Network.PolynomialApproximantSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_quadratic_sigmoid.html#a0052af6f56ecf8baca691818c8949987">SharpNeat.Network.QuadraticSigmoid</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_linear.html#a8799f818b13d6e181fb8cfd2a2544559">SharpNeat.Network.Linear</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_soft_sign_steep.html#ab58dd3f6c27a1a2b156db289feb9d0c9">SharpNeat.Network.SoftSignSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_bipolar_gaussian.html#afff2fbcf8711cfe6f77dc35581ad2b60">SharpNeat.Network.BipolarGaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_gaussian.html#a609a38abdf5af235cf783c0b2a2bc25f">SharpNeat.Network.Gaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_bipolar_sigmoid.html#adfab36bc1d0b4cabfeaf387886f9706c">SharpNeat.Network.BipolarSigmoid</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_sine.html#a7392e67ba7878955199506b15c702544">SharpNeat.Network.Sine</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_logistic_function.html#a2d544fb80e425b1ad121b875f9e5688f">SharpNeat.Network.LogisticFunction</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_logistic_function_steep.html#a5644cbc8a69b0264826985943d109158">SharpNeat.Network.LogisticFunctionSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_null_fn.html#ae98b24a71b707decef667eecceb24264">SharpNeat.Network.NullFn</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_scaled_e_l_u.html#aa1e4b04bdea71badbfd536042e56e751">SharpNeat.Network.ScaledELU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_s_re_l_u_shifted.html#a11d18332b5ef19c79855146f5ebfc21e">SharpNeat.Network.SReLUShifted</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_s_re_l_u.html#a63cfe2b5385a0727fef7f8cb2e746004">SharpNeat.Network.SReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u_shifted.html#a0f3f79773c3dd95d507a585ce966d33c">SharpNeat.Network.LeakyReLUShifted</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u.html#ae41a36f47a2e73249b230b0731f921c4">SharpNeat.Network.LeakyReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_max_minus_one.html#a352244a798c4a02bc3dde0c00bcaab9e">SharpNeat.Network.MaxMinusOne</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_re_l_u.html#a7a7cf526a2c169c2407750e74f4be7e2">SharpNeat.Network.ReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_arc_sin_h.html#ae6d74f711ea4927894ab5253c55258d6">SharpNeat.Network.ArcSinH</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_arc_tan.html#a065e102231206e1b76d6dcf3c662e0f8">SharpNeat.Network.ArcTan</a>, and <a class="el" href="class_sharp_neat_1_1_network_1_1_tan_h.html#a39d816ab83b16f52b33857912e7df5c4">SharpNeat.Network.TanH</a>.</p>

</div>
</div>
<a id="a6bf5c2eb7a6b09deb280a6e3653700db"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6bf5c2eb7a6b09deb280a6e3653700db">&#9670;&nbsp;</a></span>GetRandomAuxArgs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double [] SharpNeat.Network.IActivationFunction.GetRandomAuxArgs </td>
          <td>(</td>
          <td class="paramtype">IRandomSource&#160;</td>
          <td class="paramname"><em>rng</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>connectionWeightRange</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>For activation functions that accept auxiliary arguments; generates random initial values for aux arguments for newly added nodes (from an 'add neuron' mutation). </p>

<p>Implemented in <a class="el" href="class_sharp_neat_1_1_network_1_1_quadratic_sigmoid.html#a04e7ea87426853e5015aad12236a3621">SharpNeat.Network.QuadraticSigmoid</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep.html#a26e2bce98543337bda5679d1597b9cff">SharpNeat.Network.PolynomialApproximantSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_rbf_gaussian.html#aa6ffe92a37b41e9656cce30959f4b913">SharpNeat.Network.RbfGaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_linear.html#a96c541182acaec4aa2123ba6a08b34a9">SharpNeat.Network.Linear</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_soft_sign_steep.html#a0ca294f3166ff4714b2ea1de04c9ccaf">SharpNeat.Network.SoftSignSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_bipolar_gaussian.html#a0ee5bf2871cbe231fc9d7c3cf4c3121e">SharpNeat.Network.BipolarGaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_gaussian.html#a973fb294dddd7951b3e50068b001bf00">SharpNeat.Network.Gaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_bipolar_sigmoid.html#a16126ee75e3c1277d7a1ffa2022b17ea">SharpNeat.Network.BipolarSigmoid</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_sine.html#a2bb9e202ae1bf35df2facd842f7085ba">SharpNeat.Network.Sine</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_logistic_function.html#a5ee5ed8279389813022be7ef7e0e56f6">SharpNeat.Network.LogisticFunction</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_logistic_function_steep.html#a543dd8c17c669114894f52bc1f43da98">SharpNeat.Network.LogisticFunctionSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_null_fn.html#ab13ca5ada9eda755df8146808f6c5d4b">SharpNeat.Network.NullFn</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_s_re_l_u_shifted.html#a45d8b257fc582204030d25590d275561">SharpNeat.Network.SReLUShifted</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_s_re_l_u.html#a2ebb8e7fb16703a8bd9cee8987c61d1d">SharpNeat.Network.SReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_scaled_e_l_u.html#a98d7f4c2e58b215f4fefefda11a5f33c">SharpNeat.Network.ScaledELU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u_shifted.html#ad236b47053f4f68a418a45986d02f128">SharpNeat.Network.LeakyReLUShifted</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u.html#a6a906cf8759c0af459893814d5971c2b">SharpNeat.Network.LeakyReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_max_minus_one.html#ac66fbe22cbf8298a8c2d0fbca1302331">SharpNeat.Network.MaxMinusOne</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_re_l_u.html#a34f25b60af075a514850d36c21bd2878">SharpNeat.Network.ReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_arc_sin_h.html#a41398931fe4813b947768bdacc578a38">SharpNeat.Network.ArcSinH</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_arc_tan.html#ae5c60538e68c9931608edd9bddf8ce6c">SharpNeat.Network.ArcTan</a>, and <a class="el" href="class_sharp_neat_1_1_network_1_1_tan_h.html#abc2c87e1445c2053048d8d3fad038d26">SharpNeat.Network.TanH</a>.</p>

</div>
</div>
<a id="a2943116b401f4dff51448228ec9a6b71"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2943116b401f4dff51448228ec9a6b71">&#9670;&nbsp;</a></span>MutateAuxArgs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void SharpNeat.Network.IActivationFunction.MutateAuxArgs </td>
          <td>(</td>
          <td class="paramtype">double[]&#160;</td>
          <td class="paramname"><em>auxArgs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">IRandomSource&#160;</td>
          <td class="paramname"><em>rng</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>connectionWeightRange</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Genetic mutation for auxiliary argument data. </p>

<p>Implemented in <a class="el" href="class_sharp_neat_1_1_network_1_1_quadratic_sigmoid.html#ac3602f067c87c3c3e1be8f83e87b4fd5">SharpNeat.Network.QuadraticSigmoid</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_rbf_gaussian.html#a024a4dfec2d6edab4251f2f1b8766d1b">SharpNeat.Network.RbfGaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_polynomial_approximant_steep.html#a6baa1a15140c0d49f48ab2822bca4466">SharpNeat.Network.PolynomialApproximantSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_linear.html#a4f3692da1e1ea8aad19c265ef13f03bb">SharpNeat.Network.Linear</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_soft_sign_steep.html#ae21da48cd6264dfb10bb470f2a388a26">SharpNeat.Network.SoftSignSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_bipolar_gaussian.html#afb3ed85785751b496cd9eb91666f3852">SharpNeat.Network.BipolarGaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_gaussian.html#a5930ce6d2592813b73b8e9f47b173a3f">SharpNeat.Network.Gaussian</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_bipolar_sigmoid.html#a87a7a2ed6d7f833e18fe35056354e3a3">SharpNeat.Network.BipolarSigmoid</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_sine.html#aa05763e08202b5714f6cb6a4e1e36ba7">SharpNeat.Network.Sine</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_logistic_function.html#ab6c2ed8d1d3541bad5eedd22f7921367">SharpNeat.Network.LogisticFunction</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_logistic_function_steep.html#ac6580adcccb89fd65b3781a5e1cd289c">SharpNeat.Network.LogisticFunctionSteep</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_null_fn.html#a989dbc2df6885ee8f461251439d31f93">SharpNeat.Network.NullFn</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_s_re_l_u_shifted.html#a6a5d55536ac3281790581b6d981a670a">SharpNeat.Network.SReLUShifted</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_s_re_l_u.html#ac6d84ea80a067f4a74e6c48a40cfba37">SharpNeat.Network.SReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_scaled_e_l_u.html#a0cde810468052db41b54cfbd173b3e2f">SharpNeat.Network.ScaledELU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u_shifted.html#a343bee6ab56636796bd9a78f6acc5484">SharpNeat.Network.LeakyReLUShifted</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_leaky_re_l_u.html#a76776e42fa2e210d387a5b3f45d5a229">SharpNeat.Network.LeakyReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_max_minus_one.html#a68d827af6c1077b753cf2c5639b703fa">SharpNeat.Network.MaxMinusOne</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_re_l_u.html#a182e73f2f35e978c7b6689cb2f99b3b0">SharpNeat.Network.ReLU</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_arc_sin_h.html#a62d1ba3d15b52915aaede1ecac0ea17e">SharpNeat.Network.ArcSinH</a>, <a class="el" href="class_sharp_neat_1_1_network_1_1_arc_tan.html#aef1b1316ed9f3d4f0ad494920ab4a7be">SharpNeat.Network.ArcTan</a>, and <a class="el" href="class_sharp_neat_1_1_network_1_1_tan_h.html#aa70c8cc6d41e784fb5e2071e890dfe66">SharpNeat.Network.TanH</a>.</p>

</div>
</div>
<h2 class="groupheader">Property Documentation</h2>
<a id="ad5ff022024f524777ce2c6df1ff04672"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad5ff022024f524777ce2c6df1ff04672">&#9670;&nbsp;</a></span>AcceptsAuxArgs</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool SharpNeat.Network.IActivationFunction.AcceptsAuxArgs</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">get</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets a flag that indicates if the activation function accepts auxiliary arguments. </p>

</div>
</div>
<a id="a5581e8b834a77292473b0ea23c551302"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5581e8b834a77292473b0ea23c551302">&#9670;&nbsp;</a></span>FunctionDescription</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">string SharpNeat.Network.IActivationFunction.FunctionDescription</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">get</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets a human readable verbose description of the activation function. </p>

</div>
</div>
<a id="a63c74ef293a625aa7e35ce9d6dea7e2d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a63c74ef293a625aa7e35ce9d6dea7e2d">&#9670;&nbsp;</a></span>FunctionId</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">string SharpNeat.Network.IActivationFunction.FunctionId</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">get</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets the unique ID of the function. Stored in network XML to identify which function a network or neuron is using. </p>

</div>
</div>
<a id="ad9f4086a305d232a1f099eba8794293a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad9f4086a305d232a1f099eba8794293a">&#9670;&nbsp;</a></span>FunctionString</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">string SharpNeat.Network.IActivationFunction.FunctionString</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">get</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets a human readable string representation of the function. E.g 'y=1/x'. </p>

</div>
</div>
<hr/>The documentation for this interface was generated from the following file:<ul>
<li>src/SharpNeatLib/Network/IActivationFunction.cs</li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespace_sharp_neat.html">SharpNeat</a></li><li class="navelem"><a class="el" href="namespace_sharp_neat_1_1_network.html">Network</a></li><li class="navelem"><a class="el" href="interface_sharp_neat_1_1_network_1_1_i_activation_function.html">IActivationFunction</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
