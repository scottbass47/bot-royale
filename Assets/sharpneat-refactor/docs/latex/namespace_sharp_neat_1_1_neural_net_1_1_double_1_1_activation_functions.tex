\hypertarget{namespace_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions}{}\doxysection{Sharp\+Neat.\+Neural\+Net.\+Double.\+Activation\+Functions Namespace Reference}
\label{namespace_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions}\index{SharpNeat.NeuralNet.Double.ActivationFunctions@{SharpNeat.NeuralNet.Double.ActivationFunctions}}
\doxysubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_arc_sin_h}{Arc\+SinH}}
\begin{DoxyCompactList}\small\item\em The \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_arc_sin_h}{Arc\+SinH}} function (inverse hyperbolic sine function). \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_arc_tan}{Arc\+Tan}}
\begin{DoxyCompactList}\small\item\em The \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_arc_tan}{Arc\+Tan}} function (inverse tangent function). \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_leaky_re_l_u}{Leaky\+Re\+LU}}
\begin{DoxyCompactList}\small\item\em Leaky rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_re_l_u}{Re\+LU}}). \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_leaky_re_l_u_shifted}{Leaky\+Re\+L\+U\+Shifted}}
\begin{DoxyCompactList}\small\item\em Leaky rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_re_l_u}{Re\+LU}}). Shifted on the x-\/axis so that x=0 gives y=0.\+5, in keeping with the logistic sigmoid. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_logistic}{Logistic}}
\begin{DoxyCompactList}\small\item\em The logistic function.  \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_logistic_approximant_steep}{Logistic\+Approximant\+Steep}}
\begin{DoxyCompactList}\small\item\em The logistic function with a steepened slope, and implemented using a fast to compute approximation of exp(). See\+: \href{https://stackoverflow.com/a/412988/15703}{\texttt{ https\+://stackoverflow.\+com/a/412988/15703}} \href{https://pdfs.semanticscholar.org/35d3/2b272879a2018a2d33d982639d4be489f789.pdf}{\texttt{ https\+://pdfs.\+semanticscholar.\+org/35d3/2b272879a2018a2d33d982639d4be489f789.\+pdf}} (A Fast, Compact Approximation of the Exponential Function) \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_logistic_steep}{Logistic\+Steep}}
\begin{DoxyCompactList}\small\item\em The logistic function with a steepened slope.  \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_max_minus_one}{Max\+Minus\+One}}
\begin{DoxyCompactList}\small\item\em max(-\/1, x,) function. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_null_fn}{Null\+Fn}}
\begin{DoxyCompactList}\small\item\em Null activation function. Returns zero regardless of input. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_polynomial_approximant_steep}{Polynomial\+Approximant\+Steep}}
\begin{DoxyCompactList}\small\item\em A very close approximation of the logistic function that avoids use of exp() and is therefore typically much faster to compute, while giving an almost identical sigmoid curve. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_quadratic_sigmoid}{Quadratic\+Sigmoid}}
\begin{DoxyCompactList}\small\item\em A sigmoid formed by two sub-\/sections of the y=x$^\wedge$2 curve. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_re_l_u}{Re\+LU}}
\begin{DoxyCompactList}\small\item\em Rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_re_l_u}{Re\+LU}}). \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_scaled_e_l_u}{Scaled\+E\+LU}}
\begin{DoxyCompactList}\small\item\em Scaled Exponential Linear Unit (S\+E\+LU). \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_soft_sign_steep}{Soft\+Sign\+Steep}}
\begin{DoxyCompactList}\small\item\em The softsign sigmoid. This is a variant of softsign that has a steeper slope at and around the origin that is intended to be a similar slope to that of Logistic\+Function\+Steep. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_s_re_l_u}{S\+Re\+LU}}
\begin{DoxyCompactList}\small\item\em S-\/shaped rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_s_re_l_u}{S\+Re\+LU}}). From\+: \href{https://en.wikipedia.org/wiki/Activation_function}{\texttt{ https\+://en.\+wikipedia.\+org/wiki/\+Activation\+\_\+function}} \href{https://arxiv.org/abs/1512.07030}{\texttt{ https\+://arxiv.\+org/abs/1512.\+07030}} \mbox{[}Deep Learning with S-\/shaped Rectified Linear Activation Units\mbox{]} \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_s_re_l_u_shifted}{S\+Re\+L\+U\+Shifted}}
\begin{DoxyCompactList}\small\item\em S-\/shaped rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_s_re_l_u}{S\+Re\+LU}}). Shifted on the x-\/axis so that x=0 gives y=0.\+5, in keeping with the logistic sigmoid. From\+: \href{https://en.wikipedia.org/wiki/Activation_function}{\texttt{ https\+://en.\+wikipedia.\+org/wiki/\+Activation\+\_\+function}} \href{https://arxiv.org/abs/1512.07030}{\texttt{ https\+://arxiv.\+org/abs/1512.\+07030}} \mbox{[}Deep Learning with S-\/shaped Rectified Linear Activation Units\mbox{]} \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_tan_h}{TanH}}
\begin{DoxyCompactList}\small\item\em \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_tan_h}{TanH}} function (hyperbolic tangent function). \end{DoxyCompactList}\end{DoxyCompactItemize}
