\hypertarget{namespace_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized}{}\doxysection{Sharp\+Neat.\+Neural\+Net.\+Double.\+Activation\+Functions.\+Vectorized Namespace Reference}
\label{namespace_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized}\index{SharpNeat.NeuralNet.Double.ActivationFunctions.Vectorized@{SharpNeat.NeuralNet.Double.ActivationFunctions.Vectorized}}
\doxysubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_leaky_re_l_u}{Leaky\+Re\+LU}}
\begin{DoxyCompactList}\small\item\em Leaky rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_re_l_u}{Re\+LU}}). \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_leaky_re_l_u_shifted}{Leaky\+Re\+L\+U\+Shifted}}
\begin{DoxyCompactList}\small\item\em Leaky rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_re_l_u}{Re\+LU}}). \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_max_minus_one}{Max\+Minus\+One}}
\begin{DoxyCompactList}\small\item\em max(-\/1, x,) function. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep}{Polynomial\+Approximant\+Steep}}
\begin{DoxyCompactList}\small\item\em A very close approximation of the logistic function that avoids use of exp() and is therefore typically much faster to compute, while giving an almost identical sigmoid curve. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_quadratic_sigmoid}{Quadratic\+Sigmoid}}
\begin{DoxyCompactList}\small\item\em A sigmoid formed by two sub-\/sections of the y=x$^\wedge$2 curve. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_re_l_u}{Re\+LU}}
\begin{DoxyCompactList}\small\item\em Rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_re_l_u}{Re\+LU}}). \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_soft_sign_steep}{Soft\+Sign\+Steep}}
\begin{DoxyCompactList}\small\item\em The softsign sigmoid. This is a variant of softsign that has a steeper slope at and around the origin that is intended to be a similar slope to that of Logistic\+Function\+Steep. \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_s_re_l_u}{S\+Re\+LU}}
\begin{DoxyCompactList}\small\item\em S-\/shaped rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_s_re_l_u}{S\+Re\+LU}}). From\+: \href{https://en.wikipedia.org/wiki/Activation_function}{\texttt{ https\+://en.\+wikipedia.\+org/wiki/\+Activation\+\_\+function}} \href{https://arxiv.org/abs/1512.07030}{\texttt{ https\+://arxiv.\+org/abs/1512.\+07030}} \mbox{[}Deep Learning with S-\/shaped Rectified Linear Activation Units\mbox{]} \end{DoxyCompactList}\item 
class \mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_s_re_l_u_shifted}{S\+Re\+L\+U\+Shifted}}
\begin{DoxyCompactList}\small\item\em S-\/shaped rectified linear activation unit (\mbox{\hyperlink{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_s_re_l_u}{S\+Re\+LU}}). From\+: \href{https://en.wikipedia.org/wiki/Activation_function}{\texttt{ https\+://en.\+wikipedia.\+org/wiki/\+Activation\+\_\+function}} \href{https://arxiv.org/abs/1512.07030}{\texttt{ https\+://arxiv.\+org/abs/1512.\+07030}} \mbox{[}Deep Learning with S-\/shaped Rectified Linear Activation Units\mbox{]} \end{DoxyCompactList}\end{DoxyCompactItemize}
