\hypertarget{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep}{}\doxysection{Sharp\+Neat.\+Neural\+Net.\+Double.\+Activation\+Functions.\+Vectorized.\+Polynomial\+Approximant\+Steep Class Reference}
\label{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep}\index{SharpNeat.NeuralNet.Double.ActivationFunctions.Vectorized.PolynomialApproximantSteep@{SharpNeat.NeuralNet.Double.ActivationFunctions.Vectorized.PolynomialApproximantSteep}}


A very close approximation of the logistic function that avoids use of exp() and is therefore typically much faster to compute, while giving an almost identical sigmoid curve.  


Inheritance diagram for Sharp\+Neat.\+Neural\+Net.\+Double.\+Activation\+Functions.\+Vectorized.\+Polynomial\+Approximant\+Steep\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep}
\end{center}
\end{figure}
\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep_abae449cf9a41987a8adadf86340afa60}\label{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep_abae449cf9a41987a8adadf86340afa60}} 
double {\bfseries Fn} (double x)
\item 
\mbox{\Hypertarget{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep_a411804ba399fc39acbe78784e0bc3052}\label{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep_a411804ba399fc39acbe78784e0bc3052}} 
void {\bfseries Fn} (double\mbox{[}$\,$\mbox{]} v)
\item 
\mbox{\Hypertarget{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep_a255894e39e38f2cb6a2989abcc2d7e16}\label{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep_a255894e39e38f2cb6a2989abcc2d7e16}} 
void {\bfseries Fn} (double\mbox{[}$\,$\mbox{]} v, int start\+Idx, int end\+Idx)
\item 
\mbox{\Hypertarget{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep_ac39639707c2b905cf0f6cedefeeff0c5}\label{class_sharp_neat_1_1_neural_net_1_1_double_1_1_activation_functions_1_1_vectorized_1_1_polynomial_approximant_steep_ac39639707c2b905cf0f6cedefeeff0c5}} 
void {\bfseries Fn} (double\mbox{[}$\,$\mbox{]} v, double\mbox{[}$\,$\mbox{]} w, int start\+Idx, int end\+Idx)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
A very close approximation of the logistic function that avoids use of exp() and is therefore typically much faster to compute, while giving an almost identical sigmoid curve. 

This function was obtained from\+: \href{http://stackoverflow.com/a/34448562/15703}{\texttt{ http\+://stackoverflow.\+com/a/34448562/15703}}

This might be based on the Pade approximant\+: \href{https://en.wikipedia.org/wiki/Pad\%C3\%A9_approximant}{\texttt{ https\+://en.\+wikipedia.\+org/wiki/\+Pad\%\+C3\%\+A9\+\_\+approximant}} \href{https://math.stackexchange.com/a/107666}{\texttt{ https\+://math.\+stackexchange.\+com/a/107666}}

Or perhaps the maple minimax approximation\+: \href{http://www.maplesoft.com/support/helpJP/Maple/view.aspx?path=numapprox/minimax}{\texttt{ http\+://www.\+maplesoft.\+com/support/help\+J\+P/\+Maple/view.\+aspx?path=numapprox/minimax}}

This is a variant that has a steeper slope at and around the origin that is intended to be a similar slope to that of Logistic\+Function\+Steep.

The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/\+Sharp\+Neat\+Lib/\+Neural\+Net/\+Double/\+Activation\+Functions/\+Vectorized/Polynomial\+Approximant\+Steep.\+cs\end{DoxyCompactItemize}
